import pickle
import numpy as np
import pandas as pd
import itertools
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.autograd._functions import tensor
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


torch.cuda.empty_cache()

# Device configuration
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
num_epochs = 50
num_classes = 11
batch_size = 256
learning_rate = 0.001
in_channels = 1

#Load Data

Xd = pickle.load(open('word_data_unix.pkl', 'rb'), encoding='latin')
snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])
print(snrs)
print(mods)
X = []
lbl = []

for mod in mods:
    for snr in snrs:
        if snr < 0 or snr >18: continue
        X.append(Xd[(mod,snr)])
        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))
X = np.vstack(X)


# Partition the data
#  into training and test sets of the form we can train/test on
#  while keeping SNR and Mod labels handy for each
#np.random.seed(2016)
n_examples = X.shape[0]
n_train = int(n_examples * 0.8)
train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)
test_idx = list(set(range(0,n_examples))-set(train_idx))
X_train = X[train_idx]
X_test =  X[test_idx]
def to_onehot(yy):
    yy1 = np.zeros([len(yy), max(yy)+1])
    yy1[np.arange(len(yy)),yy] = 1
    return yy1
Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))
Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))

# Converting Numpy array to Tensor
x_train, x_test = map(torch.tensor, (X_train,X_test))
y_train, y_test = map(torch.tensor, (Y_train, Y_test))


# Creating the dataset for Training
train_ds = TensorDataset(x_train, y_train) #Training Dataset
train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True) #Training Dataloader
# Creating the dataset for Testing
test_ds = TensorDataset(x_test, y_test) #Testing Dataset
test_dl = DataLoader(test_ds, batch_size=1024,shuffle=True) #Testing Dataloader

input_shape = list(X_train.shape[1:])

#Input [N , 1 , 2, 128]
class ConvNet(nn.Module):
    def __init__(self,dr=0.5,in_channels=1,num_classes=11):
        super(ConvNet,self).__init__()
        self.pad = nn.ConstantPad2d((2,2,0,0),0)
        self.conv1 = nn.Conv2d(1,256,(1,3)) #First convolutional Layer (Input_channels, Output_channels, Kernel)
        self.drop = nn.Dropout(dr) #Zeros some of the elements of the input tensor, probability of an element to be zeroed (p=0.5)
        self.zeropad2 = nn.ConstantPad2d((2,2,0,0),0)
        self.bnorm = nn.BatchNorm2d(256)
        self.conv2 = nn.Conv2d(256,80,(2,3)) #Second convolutional Layer
        self.bnorm2 = nn.BatchNorm2d(80)
        self.Dense = nn.Linear(10560,256) # at the point will get flatten, the number of neurons required will be 10560
        self.bnorm3 = nn.BatchNorm1d(256)
        self.Dense2 = nn.Linear(256,num_classes)

    def forward(self, x):

        x = self.pad(x)
        x = F.relu(self.conv1(x))
        x = self.drop(x)
        x = self.pad(x)
        x = self.bnorm(x)
        x = F.relu(self.conv2(x))
        x = self.bnorm2(x)
        x = torch.flatten(x,1)
        x = F.relu(self.Dense(x))
        x = self.bnorm3(x)
        x = self.Dense2(x)

        return x

conv = ConvNet().to(device)

#Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(conv.parameters(), lr=learning_rate)

#Train the Model

total_step = len(train_dl)
conv.train()
for epoch in range(num_epochs):
    for i, (data, labels) in enumerate(train_dl):
        # Get data to cuda if possible
        data = data.reshape([data.size(dim=0), 1]+input_shape,[data.size(dim=0), 1]+input_shape)
        data = data.to(device)
        labels = labels.to(device)

        #forward
        output = conv(data)
        loss = criterion(output, torch.max(labels, 1)[1])


        #backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i + 1) % 100 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                 .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))
print('Finished Training')

# Test the model

with torch.no_grad():
    conv.eval()
    correct = 0
    total = 0
    for data, labels in test_dl:
        data = data.reshape([data.size(dim=0), 1]+input_shape,[data.size(dim=0), 1]+input_shape)
        data = data.to(device)
        labels = labels.to(device)

        #forward
        output = conv(data)


        _, predictions = torch.max(F.softmax(output,dim=1), 1)
        total += labels.size(0)
        correct += torch.sum(torch.argmax(labels,dim=1)==predictions).item()


print('Test Accuracy of the model on the test signals: {} %'.format(100 * correct / total))
print('Number of correct predictions is: ',correct,'Out of a possible: ',total)

#confusion matrix

cm = np.zeros((num_classes,num_classes))
with torch.no_grad():
    for i, (data, labels) in enumerate(test_dl):
        data = data.reshape([data.size(dim=0), 1] + input_shape, [data.size(dim=0), 1] + input_shape)
        data = data.to(device)
        labels = labels.to(device)
        output = conv(data)
        _, predictions = torch.max(F.softmax(output,dim=1), 1)
        labels = torch.argmax(labels, dim=1)
        cm += confusion_matrix(labels.cpu().numpy(), predictions.detach().cpu().numpy())

# print(cm)


ax = plt.subplot()
sns.heatmap(cm, annot=True, fmt='g', ax=ax, cmap=plt.cm.Blues)
# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Radio Signal Classification Confusion Matrix');
ax.xaxis.set_ticklabels(['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM'])
ax.yaxis.set_ticklabels(['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM'])

plt.show()


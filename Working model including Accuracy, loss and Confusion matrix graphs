import pickle
import numpy as np
import pandas as pd
import itertools
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.autograd._functions import tensor
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method
from absl import app,flags

torch.cuda.empty_cache()

# Device configuration
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
num_epochs = 25
num_classes = 11
batch_size = 256
learning_rate = 0.001
in_channels = 1

#Load Data

Xd = pickle.load(open('word_data_unix.pkl', 'rb'), encoding='latin')
snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])
print(snrs)
print(mods)
X = []
lbl = []

for mod in mods:
    for snr in snrs:
        if snr < 0 or snr >18: continue
        X.append(Xd[(mod,snr)])
        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))
X = np.vstack(X)


# Partition the data
#  into training and test sets of the form we can train/test on
#  while keeping SNR and Mod labels handy for each
#np.random.seed(2016)
n_examples = X.shape[0]
n_train = int(n_examples * 0.8)
train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)
test_idx = list(set(range(0,n_examples))-set(train_idx))
X_train = X[train_idx]
X_test =  X[test_idx]
def to_onehot(yy):
    yy1 = np.zeros([len(yy), max(yy)+1])
    yy1[np.arange(len(yy)),yy] = 1
    return yy1
Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))
Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))

# Converting Numpy array to Tensor
x_train, x_test = map(torch.tensor, (X_train,X_test))
y_train, y_test = map(torch.tensor, (Y_train, Y_test))


# Creating the dataset for Training
train_ds = TensorDataset(x_train, y_train) #Training Dataset
train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True) #Training Dataloader
# Creating the dataset for Testing
test_ds = TensorDataset(x_test, y_test) #Testing Dataset
test_dl = DataLoader(test_ds, batch_size=1024,shuffle=True) #Testing Dataloader

input_shape = list(X_train.shape[1:])

#Input [N , 1 , 2, 128]
class ConvNet(nn.Module):
    def __init__(self,dr=0.5,in_channels=1,num_classes=11):
        super(ConvNet,self).__init__()
        self.pad = nn.ConstantPad2d((2,2,0,0),0) #[N,1,2,132]
        self.conv1 = nn.Conv2d(1,16,(1,3)) #[N,32,2,130]
        self.drop = nn.Dropout(dr) #[N,16,2,130]
        self.zeropad2 = nn.ConstantPad2d((2,2,0,0),0) #[N,16,2,134]
        self.bnorm = nn.BatchNorm2d(16)
        self.conv2 = nn.Conv2d(16,8,(2,3)) #Second convolutional Layer [N,16,1,132]
        self.Dense = nn.Linear(1056,256) # at the point will get flatten, the number of neurons required will be 8*1*132 = 1056
        self.Dense2 = nn.Linear(256,num_classes)


    def forward(self, x):

        x = self.pad(x)
        x = F.relu(self.conv1(x))
        x = self.drop(x)
        x = self.pad(x)
        x = self.bnorm(x)
        x = F.relu(self.conv2(x))
        x = torch.flatten(x,1)
        x = F.relu(self.Dense(x))
        x = self.Dense2(x)

        return x
conv = ConvNet().to(device)

#Loss and optimizer
loss_fn = nn.CrossEntropyLoss(label_smoothing=0)
optimizer = torch.optim.Adam(conv.parameters(), lr=learning_rate)

#Train the Model
train_loss=0
train_accu=[]
train_losses=[]
def train(epoch):
    print('\nEpoch : %d'%epoch)
    total_step = len(train_dl)
    conv.train()
    running_loss=0
    correct=0
    total=0
    #train_loss=0

    for i, (data, labels) in enumerate(train_dl):
            # Get data to cuda if possible
            data = data.reshape([data.size(dim=0), 1] + input_shape, [data.size(dim=0), 1] + input_shape)
            data = data.to(device)
            labels = labels.to(device)

            # forward
            output = conv(data)
            loss = loss_fn(output, torch.max(labels, 1)[1])

            # backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predictions = torch.max(F.softmax(output, dim=1), 1)
            total += labels.size(0)
            correct += torch.sum(torch.argmax(labels, dim=1) == predictions).item()
    train_loss=running_loss/len(train_dl)
    accu=100.*correct/total

    train_accu.append(accu)
    train_losses.append(train_loss)
    print('Train Loss: %.3f | Accuracy: %.3f' % (train_loss, accu))

# Test the model
eval_losses=[]
eval_accu=[]
def test(epoch):
    conv.eval()

    running_loss=0
    correct = 0
    total = 0
    with torch.no_grad():
        for data, labels in test_dl:
            data = data.reshape([data.size(dim=0), 1] + input_shape, [data.size(dim=0), 1] + input_shape)
            data = data.to(device)
            labels = labels.to(device)
            #data = fast_gradient_method(conv, data, eps=0.05, norm=2, clip_min=0., clip_max=1.)

            # forward
            output = conv(data)

            loss = loss_fn(output, torch.max(labels, 1)[1])
            running_loss+=loss.item()

            _, predictions = torch.max(F.softmax(output, dim=1), 1)
            total += labels.size(0)
            correct += torch.sum(torch.argmax(labels, dim=1) == predictions).item()

    test_loss = running_loss / len(test_dl)
    accu = 100. * correct / total

    eval_losses.append(test_loss)
    eval_accu.append(accu)

    print('Test Loss: %.3f | Accuracy: %.3f' % (test_loss, accu))

epochs = 7
for epoch in range(1,epochs+1):
    train(epoch)
    test(epoch)

plt.plot(train_accu,'-o')
plt.plot(eval_accu,'-o')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['Train','Valid'])
plt.title('Train vs Valid Accuracy')

plt.show()

plt.plot(train_losses,'-o')
plt.plot(eval_losses,'-o')
plt.xlabel('epoch')
plt.ylabel('losses')
plt.legend(['Train','Valid'])
plt.title('Train vs Valid Losses')

plt.show()
#confusion matrix

cm = np.zeros((num_classes,num_classes))

for i, (data, labels) in enumerate(test_dl):
    data = data.reshape([data.size(dim=0), 1] + input_shape, [data.size(dim=0), 1] + input_shape)
    data = data.to(device)
    labels = labels.to(device)
    #data = fast_gradient_method(conv, data, eps=0.05, norm=2, clip_min=0., clip_max=1.)
    output = conv(data)
    _, predictions = torch.max(F.softmax(output,dim=1), 1)
    labels = torch.argmax(labels, dim=1)
    cm += confusion_matrix(labels.cpu().numpy(), predictions.detach().cpu().numpy())

print(cm)


ax = plt.subplot()
sns.heatmap(cm, annot=True, fmt='g', ax=ax, cmap=plt.cm.Blues)
# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Radio Signal Classification Confusion Matrix');
ax.xaxis.set_ticklabels(['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM'])
ax.yaxis.set_ticklabels(['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM'])

plt.show()

import pickle
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.autograd._functions import tensor
from torch.utils.data import TensorDataset, DataLoader

# Device configuration
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
num_epochs = 1
num_classes = 11
batch_size = 256
learning_rate = 0.001
in_channels = 1

#Load Data

Xd = pickle.load(open('word_data_unix.pkl', 'rb'), encoding='latin')
snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])
X = []
lbl = []
for mod in mods:
    for snr in snrs:
        X.append(Xd[(mod,snr)])
        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))
X = np.vstack(X)

# Partition the data
#  into training and test sets of the form we can train/test on
#  while keeping SNR and Mod labels handy for each
#np.random.seed(2016)
n_examples = X.shape[0]
n_train = int(n_examples * 0.8)
train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)
test_idx = list(set(range(0,n_examples))-set(train_idx))
X_train = X[train_idx]
X_test =  X[test_idx]
def to_onehot(yy):
    yy1 = np.zeros([len(yy), max(yy)+1])
    yy1[np.arange(len(yy)),yy] = 1
    return yy1
Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))
Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))

# Converting Numpy array to Tensor
x_train, x_test = map(torch.tensor, (X_train,X_test))
y_train, y_test = map(torch.tensor, (Y_train, Y_test))

# Creating the dataset for Training
train_ds = TensorDataset(x_train, y_train) #Training Dataset
train_dl = DataLoader(train_ds, batch_size=batch_size) #Training Dataloader
# Creating the dataset for Testing
test_ds = TensorDataset(x_test, y_test) #Testing Dataset
test_dl = DataLoader(test_ds, batch_size=batch_size) #Testing Dataloader

input_shape = torch.Tensor(x_train.shape[1:])

dataiter = iter(train_dl)
data, labels = dataiter.next()
data = data.unsqueeze(1)
print(data.shape)


pad = nn.ConstantPad2d((2,2,0,0),0)
conv1 = nn.Conv2d(in_channels=1,out_channels=256,kernel_size=(1,3)) #First convolutional Layer (Input_channels, Output_channels, Kernel)
drop = nn.Dropout(p=0.5) #Zeros some of the elements of the input tensor, probability of an element to be zeroed (p=0.5)
zeropad2 = nn.ConstantPad2d((2,2,0,0),0)
conv2 = nn.Conv2d(in_channels=256,out_channels=80,kernel_size=(2,3)) #Second convolutional Layer
flatten = nn.Flatten() #80*132 = 10560
Dense = nn.Linear(10560,256) # at the point will get flatten, the number of neurons required will be 10560
Dense2 = nn.Linear(256,num_classes)


x = pad(data)
print(x.shape)
x = conv1(data)
print(x.shape)
x = drop(data)
print(x.shape)
x = zeropad2(data)
print(x.shape)
x = conv2(data)
print(x.shape)
x = data.reshape(data.size(0),-1)
print(x.shape)
x = Dense(data)
print(x.shape)
x = Dense2(data)
print(x.shape)


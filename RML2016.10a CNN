import pickle
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import TensorDataset, DataLoader

# Device configuration
#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
num_epochs = 1
num_classes = 11
batch_size = 256
learning_rate = 0.001
num_classes = 11
in_channel = 1

#Load Data

Xd = pickle.load(open('word_data_unix.pkl', 'rb'), encoding='latin')
snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])
X = []
lbl = []
for mod in mods:
    for snr in snrs:
        X.append(Xd[(mod,snr)])
        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))
X = np.vstack(X)

# Partition the data
#  into training and test sets of the form we can train/test on
#  while keeping SNR and Mod labels handy for each
#np.random.seed(2016)
n_examples = X.shape[0]
n_train = int(n_examples * 0.8)
train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)
test_idx = list(set(range(0,n_examples))-set(train_idx))
X_train = X[train_idx]
X_test =  X[test_idx]
def to_onehot(yy):
    yy1 = np.zeros([len(yy), max(yy)+1])
    yy1[np.arange(len(yy)),yy] = 1
    return yy1
Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))
Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))

# Converting Numpy array to Tensor
x_train, x_test = map(torch.tensor, (X_train,X_test))
y_train, y_test = map(torch.tensor, (Y_train, Y_test))

# Creating the dataset for Training
train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=batch_size)
# Creating the dataset for Testing
test_ds = TensorDataset(x_test, y_test)
test_dl = DataLoader(test_ds, batch_size=batch_size)

input_shape = list(X_train.shape[1:]) #X_train = [171000, 2,128]    Input_shape = [2,128]
print(input_shape)

X_fd = torch.Tensor(X)
X_fd = X_fd.reshape([X_fd.size(dim=0), 1]+input_shape,[X_fd.size(dim=0), 1]+input_shape) #Input (batch size, input channel, 2, 128
print(X_fd.shape)

#conv2d(input_channel, output_channel, kernel_size)
#Input [N , 1 , 2, 128]
class ConvNet(nn.Module):
    def __init__(self,dr=0.5,in_channels=1,num_classes=11):
        super(ConvNet,self).__init__()
        self.pad = nn.ConstantPad2d((2,2,0,0),0)
        self.conv1 = nn.Conv2d(in_channels=1,out_channels=256,kernel_size=(1,3)) #First convolutional Layer (Input_channels, Output_channels, Kernel)
        self.drop = nn.Dropout(dr) #Zeros some of the elements of the input tensor, probability of an element to be zeroed (p=0.5)
        self.zeropad2 = nn.ConstantPad2d((2,2,0,0),0)
        self.conv2 = nn.Conv2d(in_channels=256,out_channels=80,kernel_size=(2,3)) #Second convolutional Layer
        self.flatten = nn.Flatten() #80*132 = 10560
        self.Dense = nn.Linear(10560,256) # at the point will get flatten, the number of neurons required will be 10560
        self.Dense2 = nn.Linear(256,num_classes)

    def forward(self, x):
        x = self.pad(x) #pad
        x = F.relu(self.conv1(x)) #conv1
        x = self.drop(x) #dropout
        x = self.zeropad2(x) #zeropadding2
        x = F.relu(self.conv2(x)) #conv2
        x = x.reshape(x.size(0),-1) #flatten
        x = self.Dense(x) #Linear
        x = self.Dense2(x)

        return x

conv = ConvNet()
x = torch.randn(256,1,2,128)
print(conv(x).shape)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(conv.parameters(), lr=learning_rate)
